{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liruizhi/opt/anaconda3/lib/python3.8/site-packages/paddlenlp/transformers/funnel/modeling.py:30: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working\n",
      "  from collections import Iterable\n"
     ]
    }
   ],
   "source": [
    "# 导入所需的第三方库\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "from functools import partial\n",
    "import random\n",
    "import time\n",
    "import inspect\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "from paddle.io import IterableDataset\n",
    "from paddle.utils.download import get_path_from_url\n",
    "from paddle.io import Dataset\n",
    "# 导入paddlenlp所需的相关包\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.data import Pad, Stack, Tuple\n",
    "from paddlenlp.datasets import MapDataset\n",
    "from paddlenlp.datasets import DatasetBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-03-23 11:36:12,994] [    INFO]\u001b[0m - Downloading http://bj.bcebos.com/paddlenlp/models/transformers/bert/bert-wwm-ext-chinese.pdparams and saved to /Users/liruizhi/.paddlenlp/models/bert-wwm-ext-chinese\u001b[0m\n",
      "\u001b[32m[2022-03-23 11:36:12,996] [    INFO]\u001b[0m - Downloading bert-wwm-ext-chinese.pdparams from http://bj.bcebos.com/paddlenlp/models/transformers/bert/bert-wwm-ext-chinese.pdparams\u001b[0m\n",
      "100%|██████████| 390M/390M [06:31<00:00, 1.05MB/s]   \n",
      "\u001b[32m[2022-03-23 11:42:54,037] [    INFO]\u001b[0m - Downloading http://bj.bcebos.com/paddlenlp/models/transformers/bert/bert-wwm-ext-chinese-vocab.txt and saved to /Users/liruizhi/.paddlenlp/models/bert-wwm-ext-chinese\u001b[0m\n",
      "\u001b[32m[2022-03-23 11:42:54,039] [    INFO]\u001b[0m - Downloading bert-wwm-ext-chinese-vocab.txt from http://bj.bcebos.com/paddlenlp/models/transformers/bert/bert-wwm-ext-chinese-vocab.txt\u001b[0m\n",
      "100%|██████████| 107k/107k [00:00<00:00, 413kB/s] \n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"bert-wwm-ext-chinese\"\n",
    "# 只需指定想要使用的模型名称和文本分类的类别数即可完成Fine-tune网络定义，通过在预训练模型后拼接上一个全连接网络（Full Connected）进行分类\n",
    "model = ppnlp.transformers.BertForSequenceClassification.from_pretrained(MODEL_NAME, num_classes=44) \n",
    "# 定义模型对应的tokenizer，tokenizer可以把原始输入文本转化成模型model可接受的输入数据格式。需注意tokenizer类要与选择的模型相对应，具体可以查看PaddleNLP相关文档\n",
    "tokenizer = ppnlp.transformers.BertTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CHIPCTCDataSet(Dataset):\n",
    "    def __init__(self, data_path, label_path):\n",
    "        # 加载标签辞典\n",
    "        self.label2id = self._load_label_dict(label_path)\n",
    "        # 加载数据集\n",
    "        self.data = self._load_data_from_source(data_path)\n",
    "        # 加载标签列表\n",
    "        self.label_list = list(self.label2id.keys())\n",
    "        \n",
    "    def _load_label_dict(self, label_path):\n",
    "        with open(label_path, 'r', encoding='utf-8') as f:\n",
    "            lines = [line.strip().split('\\t', maxsplit=1)\n",
    "                     for line in f.readlines()]\n",
    "            lines = [(line[0], int(line[1])) for line in lines]\n",
    "            label_dict = dict(lines)\n",
    "        return label_dict\n",
    "    \n",
    "    def _load_data_from_source(self, data_path):\n",
    "        data_set = []\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                id, label, text = line.strip().split(',', maxsplit=2)\n",
    "                example = {'text': text, 'label':self.label2id[label]}\n",
    "                data_set.append(example)\n",
    "        return data_set\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载训练和验证集\n",
    "train_ds = CHIPCTCDataSet('/Users/liruizhi/Desktop/毕设数据处理/train_clean.csv', '/Users/liruizhi/Desktop/毕设数据处理/CHIP-CTC/CHIP-CTC_label_dict.txt')\n",
    "label2id = train_ds.label2id\n",
    "train_ds = MapDataset(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example(example, tokenizer, max_seq_length=128, is_test=False):\n",
    "    encoded_inputs = tokenizer(text=example['text'], max_seq_len=max_seq_length)\n",
    "    input_ids = encoded_inputs['input_ids']\n",
    "    token_type_ids = encoded_inputs['token_type_ids']\n",
    "    \n",
    "    if not is_test:\n",
    "        label = np.array([example['label']], dtype='int64')\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        return input_ids, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据加载函数dataloader\n",
    "def create_dataloader(dataset,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None,\n",
    "                      trans_fn=None):\n",
    "    \n",
    "    # 疑问点：trans_fn = convert_example\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    # 训练数据集随机打乱，测试数据集不打乱\n",
    "    if mode == 'train':\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        batch_sampler = paddle.io.BatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return paddle.io.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数设置：\n",
    "# 批处理大小，显存如若不足的话可以适当改小该值  \n",
    "batch_size = 128\n",
    "# 文本序列最大截断长度，需要根据文本具体长度进行确定，最长不超过512。\n",
    "max_seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据处理成模型可读入的数据格式\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length)\n",
    "\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack()  # labels\n",
    "): [data for data in fn(samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集迭代器\n",
    "train_data_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证集迭代器\n",
    "# dev_data_loader = create_dataloader(\n",
    "#     dev_ds,\n",
    "#     mode='dev',\n",
    "#     batch_size=batch_size,\n",
    "#     batchify_fn=batchify_fn,\n",
    "#     trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参，loss，优化器等\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "# 定义训练配置参数：\n",
    "# 定义训练过程中的最大学习率\n",
    "learning_rate = 4e-5\n",
    "# 训练轮次\n",
    "epochs = 4\n",
    "# 学习率预热比例\n",
    "warmup_proportion = 0.1\n",
    "# 权重衰减系数，类似模型正则项策略，避免模型过拟合\n",
    "weight_decay = 0.01\n",
    "\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\n",
    "\n",
    "# AdamW优化器\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    apply_decay_param_fun=lambda x: x in [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ])\n",
    "\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()  # 交叉熵损失函数\n",
    "metric = paddle.metric.Accuracy()              # accuracy评价指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型训练验证评估函数\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))  # 输出验证集上评估效果\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    return accu  # 返回准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<paddle.fluid.core_avx.Generator at 0x7fc035561ab0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 固定随机种子便于结果的复现\n",
    "seed = 1024\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "paddle.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3880a28da7f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-244>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad_tensor, retain_graph)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/paddle/fluid/wrapped_decorator.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mwrapped_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/paddle/fluid/framework.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m         assert in_dygraph_mode(\n\u001b[1;32m    228\u001b[0m         ), \"We only support '%s()' in dynamic graph mode, please call 'paddle.disable_static()' to enter dynamic graph mode.\" % func.__name__\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/varbase_patch_methods.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad_tensor, retain_graph)\u001b[0m\n\u001b[1;32m    246\u001b[0m                                           framework._dygraph_tracer())\n\u001b[1;32m    247\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m                 core.dygraph_run_backward([self], [grad_tensor], retain_graph,\n\u001b[0m\u001b[1;32m    249\u001b[0m                                           framework._dygraph_tracer())\n\u001b[1;32m    250\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 模型训练：\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "#save_dir = \"checkpoint\"\n",
    "#if not  os.path.exists(save_dir):\n",
    "    #os.makedirs(save_dir)\n",
    "\n",
    "pre_accu=0\n",
    "accu=0\n",
    "global_step = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, segment_ids, labels = batch\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0 :\n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\" % (global_step, epoch, step, loss, acc))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_grad()\n",
    "    # 每轮结束对验证集进行评估\n",
    "#     accu = evaluate(model, criterion, metric, dev_data_loader)\n",
    "#     print(accu)\n",
    "#     if accu > pre_accu:\n",
    "#         # 保存较上一轮效果更优的模型参数\n",
    "#         save_param_path = os.path.join(save_dir, 'model_state.pdparams')  # 保存模型参数\n",
    "#         paddle.save(model.state_dict(), save_param_path)\n",
    "#         pre_accu=accu\n",
    "    \n",
    "# tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
