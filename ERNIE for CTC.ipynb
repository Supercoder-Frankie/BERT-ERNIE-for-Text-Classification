{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "from paddle.io import Dataset\n",
    "import paddle.nn.functional as F\n",
    "import paddlenlp\n",
    "from paddlenlp.datasets import MapDataset\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CHIPCTCDataSet(Dataset):\n",
    "    def __init__(self, data_path, label_path, mode):\n",
    "        # 加载标签辞典\n",
    "        self.label2id = self._load_label_dict(label_path)\n",
    "        # 加载数据集\n",
    "        self.data = self._load_data_from_source(data_path, mode)\n",
    "        # 加载标签列表\n",
    "        self.label_list = list(self.label2id.keys())\n",
    "        \n",
    "    def _load_label_dict(self, label_path):\n",
    "        with open(label_path, 'r', encoding='utf-8') as f:\n",
    "            lines = [line.strip().split('\\t', maxsplit=1)\n",
    "                     for line in f.readlines()]\n",
    "            lines = [(line[0], int(line[1])) for line in lines]\n",
    "            label_dict = dict(lines)\n",
    "        return label_dict\n",
    "    \n",
    "    def _load_data_from_source(self, data_path, mode):\n",
    "        data_set = []\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            if mode == 'train':\n",
    "                for line in f.readlines():\n",
    "                    id, label, text = line.strip().split(',', maxsplit=2)\n",
    "                    example = {'text': text, 'label':self.label2id[label]}\n",
    "                    data_set.append(example)\n",
    "            else:\n",
    "                for line in f.readlines():\n",
    "                    id, text = line.strip().split(',', maxsplit=1)\n",
    "                    example = {'text': text}\n",
    "                    data_set.append(example)   \n",
    "        return data_set\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Dataset = CHIPCTCDataSet('/Users/liruizhi/Desktop/毕设数据处理/train_clean.csv', '/Users/liruizhi/Desktop/毕设数据处理/CHIP-CTC/CHIP-CTC_label_dict.txt', mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset.data[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将数据集转化为模型的输入格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-03-24 11:09:27,777] [    INFO]\u001b[0m - Already cached /Users/liruizhi/.paddlenlp/models/ernie-1.0/vocab.txt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'ernie-1.0'\n",
    "tokenizer = paddlenlp.transformers.ErnieTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example(example, tokenizer, max_seq_length=128, is_test=False):\n",
    "    encoded_inputs = tokenizer(text=example['text'])\n",
    "    input_ids = encoded_inputs['input_ids']\n",
    "    token_type_ids = encoded_inputs['token_type_ids']\n",
    "    \n",
    "    if not is_test:\n",
    "        label = np.array([example['label']], dtype='int64')\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        return input_ids, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dataset, mode='train', batch_size=1, batchify_fn=None, trans_fn=None):\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "        \n",
    "    shuffle=True if mode=='train' else False\n",
    "    if mode == 'train':\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        batch_sampler = paddle.io.BatchSampler(dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    return paddle.io.DataLoader(dataset=dataset, \n",
    "                                batch_sampler=batch_sampler, \n",
    "                                return_list=True, \n",
    "                                collate_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(pad_val=tokenizer.pad_token_id, axis=0),\n",
    "    Pad(pad_val=tokenizer.pad_token_type_id, axis=0),\n",
    "    Stack(dtype='int64')\n",
    "): [data for data in fn(samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paddle.device.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErnieForSequenceClassification(paddle.nn.Layer):\n",
    "    def __init__(self, MODEL_NAME, num_class=44, dropout=None):\n",
    "        super(ErnieForSequenceClassification, self).__init__()\n",
    "        # 加载预训练好的ernie，只需要指定一个名字就可以\n",
    "        self.ernie = paddlenlp.transformers.ErnieModel.from_pretrained(MODEL_NAME)\n",
    "        self.dropout = nn.Dropout(ropout if dropout is not None else self.ernie.config[\"hidden_dropout_prob\"])\n",
    "        self.classifier = nn.Linear(self.ernie.config[\"hidden_size\"], num_class)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, position_ids=None, attention_mask=None):\n",
    "        _, pooled_output = self.ernie(\n",
    "            input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            attention_mask=attention_mask)\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参设置\n",
    "n_epochs = 5\n",
    "batch_size = 128\n",
    "#max_seq_length = 128\n",
    "n_classes=44\n",
    "dropout_rate = None\n",
    "\n",
    "learning_rate = 5e-5\n",
    "warmup_proportion = 0.1\n",
    "weight_decay = 0.01\n",
    "\n",
    "MODEL_NAME = \"ernie-tiny\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集，构造DataLoader\n",
    "train_set = CHIPCTCDataSet('/Users/liruizhi/Desktop/毕设数据处理/train_clean.csv', '/Users/liruizhi/Desktop/毕设数据处理/CHIP-CTC/CHIP-CTC_label_dict.txt', mode='train')\n",
    "label2id = train_set.label2id\n",
    "train_set = MapDataset(train_set)\n",
    "\n",
    "dev_set = CHIPCTCDataSet('/Users/liruizhi/Desktop/毕设数据处理/dev_clean.csv', '/Users/liruizhi/Desktop/毕设数据处理/CHIP-CTC/CHIP-CTC_label_dict.txt', mode='train')\n",
    "label2id = dev_set.label2id\n",
    "dev_set = MapDataset(dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Disease': 0,\n",
       " 'Multiple': 1,\n",
       " 'Therapy or Surgery': 2,\n",
       " 'Consent': 3,\n",
       " 'Diagnostic': 4,\n",
       " 'Laboratory Examinations': 5,\n",
       " 'Pregnancy-related Activity': 6,\n",
       " 'Age': 7,\n",
       " 'Pharmaceutical Substance or Drug': 8,\n",
       " 'Risk Assessment': 9,\n",
       " 'Allergy Intolerance': 10,\n",
       " 'Enrollment in other studies': 11,\n",
       " 'Researcher Decision': 12,\n",
       " 'Compliance with Protocol': 13,\n",
       " 'Organ or Tissue Status': 14,\n",
       " 'Sign': 15,\n",
       " 'Addictive Behavior': 16,\n",
       " 'Capacity': 17,\n",
       " 'Life Expectancy': 18,\n",
       " 'Symptom': 19,\n",
       " 'Neoplasm Status': 20,\n",
       " 'Device': 21,\n",
       " 'Special Patient Characteristic': 22,\n",
       " 'Non-Neoplasm Disease Stage': 23,\n",
       " 'Data Accessible': 24,\n",
       " 'Encounter': 25,\n",
       " 'Diet': 26,\n",
       " 'Smoking Status': 27,\n",
       " 'Literacy': 28,\n",
       " 'Oral related': 29,\n",
       " 'Healthy': 30,\n",
       " 'Address': 31,\n",
       " 'Blood Donation': 32,\n",
       " 'Gender': 33,\n",
       " 'Receptor Status': 34,\n",
       " 'Nursing': 35,\n",
       " 'Exercise': 36,\n",
       " 'Education': 37,\n",
       " 'Sexual related': 38,\n",
       " 'Disabilities': 39,\n",
       " 'Alcohol Consumer': 40,\n",
       " 'Bedtime': 41,\n",
       " 'Ethnicity': 42,\n",
       " 'Ethical Audit': 43}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partial是Python语言的偏函数，支持更方便的在已有函数基础上定义指定参数值的新函数\n",
    "trans_func = partial(convert_example, tokenizer=tokenizer)\n",
    "train_data_loader = create_dataloader(train_set, mode=\"train\", batch_size=batch_size, batchify_fn=batchify_fn, trans_fn=trans_func)\n",
    "dev_data_loader = create_dataloader(dev_set, mode=\"train\", batch_size=batch_size, batchify_fn=batchify_fn, trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义参数并进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-03-24 11:09:40,441] [    INFO]\u001b[0m - Already cached /Users/liruizhi/.paddlenlp/models/ernie-tiny/ernie_tiny.pdparams\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 检测是否可以使用GPU，如果可以优先使用GPU\n",
    "use_gpu = True if paddle.device.get_device().startswith(\"gpu\") else False\n",
    "if use_gpu:\n",
    "    paddle.device.set_device('gpu:0')\n",
    "    \n",
    "# 加载预训练模型ERNIE\n",
    "\n",
    "# 加载用于文本分类的fune-tuning网络，不同的任务有不同的对应函数，详细可以查阅ERNIE的文档\n",
    "model =  ErnieForSequenceClassification(MODEL_NAME, num_class=n_classes, dropout=dropout_rate)\n",
    "\n",
    "# 设置优化器，LinearDecayWithWarmup是一个周期性衰减的函数，并且在初始训练的时候才用热启动策略（较小学习率，逐渐上升），避免前期训练过于震荡\n",
    "num_training_steps = len(train_data_loader) * n_epochs\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    apply_decay_param_fun=lambda x: x in [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-cc1ce41167ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-cc1ce41167ad>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# 参数更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-341>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/base.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_switch_tracer_mode_guard_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-339>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/paddle/fluid/wrapped_decorator.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mwrapped_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/paddle/fluid/framework.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m         assert in_dygraph_mode(\n\u001b[1;32m    228\u001b[0m         ), \"We only support '%s()' in dynamic graph mode, please call 'paddle.disable_static()' to enter dynamic graph mode.\" % func.__name__\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/paddle/optimizer/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    419\u001b[0m                     \u001b[0mparams_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m             optimize_ops = self._apply_optimize(\n\u001b[0m\u001b[1;32m    422\u001b[0m                 loss=None, startup_program=None, params_grads=params_grads)\n\u001b[1;32m    423\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/paddle/optimizer/optimizer.py\u001b[0m in \u001b[0;36m_apply_optimize\u001b[0;34m(self, loss, startup_program, params_grads)\u001b[0m\n\u001b[1;32m    889\u001b[0m                     params_grads['params'] = self.append_regularization_ops(\n\u001b[1;32m    890\u001b[0m                         params_grads['params'], self.regularization)\n\u001b[0;32m--> 891\u001b[0;31m                 \u001b[0moptimize_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_optimization_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mprogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/paddle/optimizer/adamw.py\u001b[0m in \u001b[0;36m_create_optimization_pass\u001b[0;34m(self, parameters_and_grads)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_optimization_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters_and_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m         optimize_ops = super(\n\u001b[0m\u001b[1;32m    372\u001b[0m             AdamW, self)._create_optimization_pass(parameters_and_grads)\n\u001b[1;32m    373\u001b[0m         \u001b[0;31m# In dygraph mode, clear _lr_to_coeff after applied gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/paddle/optimizer/optimizer.py\u001b[0m in \u001b[0;36m_create_optimization_pass\u001b[0;34m(self, parameters_and_grads)\u001b[0m\n\u001b[1;32m    694\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mparam_and_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_gradient\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_optimize_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_block\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_and_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mparam_and_grad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters_and_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/paddle/optimizer/adamw.py\u001b[0m in \u001b[0;36m_append_optimize_op\u001b[0;34m(self, block, param_and_grad)\u001b[0m\n\u001b[1;32m    299\u001b[0m                 self._beta2, Variable) else self._beta2.numpy().item(0)\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m             _, _, _, _, _, _ = _C_ops.adamw(\n\u001b[0m\u001b[1;32m    302\u001b[0m                 \u001b[0mparam_and_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_and_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoment1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoment2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0mbeta1_pow_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2_pow_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaster_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_and_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 定义统计指标\n",
    "metric = paddle.metric.Accuracy()\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "\n",
    "def evaluate(model, metric, data_loader):\n",
    "    model.eval()\n",
    "    # 每次使用测试集进行评估时，先重置掉之前的metric的累计数据，保证只是针对本次评估。\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        # 获取数据\n",
    "        input_ids, segment_ids, labels = batch\n",
    "        # 执行前向计算\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        # 计算损失\n",
    "        loss = F.cross_entropy(input=logits, label=labels)\n",
    "        loss= paddle.mean(loss)\n",
    "        losses.append(loss.numpy())\n",
    "        # 统计准确率指标\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\n",
    "    metric.reset()\n",
    "\n",
    "def train(model):\n",
    "    global_step=0\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_data_loader, start=1):\n",
    "            # 获取数据 数据来源\n",
    "            input_ids, segment_ids, labels = batch\n",
    "            # 模型前向计算 （将数据输入模型）\n",
    "            logits = model(input_ids, segment_ids)\n",
    "            loss = F.cross_entropy(input=logits, label=labels)\n",
    "            loss = paddle.mean(loss)\n",
    "\n",
    "            # 统计指标\n",
    "            probs = F.softmax(logits, axis=1)\n",
    "            correct = metric.compute(probs, labels)\n",
    "            metric.update(correct)\n",
    "            acc = metric.accumulate()\n",
    "            \n",
    "            # 打印中间训练结果\n",
    "            global_step += 1\n",
    "            if global_step % 10 == 0 :\n",
    "                print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\" % (global_step, epoch, step, loss, acc))\n",
    "            \n",
    "            # 参数更新\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_grad()\n",
    "        \n",
    "        # 模型评估\n",
    "        evaluate(model, metric, test_data_loader)\n",
    "            \n",
    "train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, id2label, batch_size=128):\n",
    "    examples = []\n",
    "    # 数据处理\n",
    "    for text in data:\n",
    "        input_ids, token_type_ids = convert_example(\n",
    "            text,\n",
    "            is_test=True)\n",
    "        examples.append((input_ids, token_type_ids))\n",
    "    \n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(pad_val=tokenizer.pad_token_id, axis=0),\n",
    "        Pad(pad_val=tokenizer.pad_token_type_id, axis=0)):fn(samples)\n",
    "    \n",
    "    # 将数据按照batch_size进行切分\n",
    "    batches = []\n",
    "    one_batch = []\n",
    "    for example in examples:\n",
    "        one_batch.append(example)\n",
    "        if len(one_batch) == batch_size:\n",
    "            batches.append(one_batch)\n",
    "            one_batch = []\n",
    "    if one_batch:\n",
    "        batches.append(one_batch)\n",
    "    \n",
    "     # 使用模型预测数据，并返回结果\n",
    "    results = []\n",
    "    model.eval()\n",
    "    for batch in batches:\n",
    "        input_ids, token_type_ids = batchify_fn(batch)\n",
    "        input_ids = paddle.to_tensor(input_ids)\n",
    "        token_type_ids = paddle.to_tensor(token_type_ids)\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        idx = paddle.argmax(probs, axis=1).numpy()\n",
    "        idx = idx.tolist()\n",
    "        labels = [id2label[i] for i in idx]\n",
    "        results.extend(labels)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_set = CHIPCTCDataSet('/Users/liruizhi/Desktop/毕设数据处理/test.csv', '/Users/liruizhi/Desktop/毕设数据处理/CHIP-CTC/CHIP-CTC_label_dict.txt', mode='test')\n",
    "data = [{\"text\":\"2.年龄1-14岁；\"}]\n",
    "\n",
    "id2label = dict([(items[1], items[0]) for items in label2id.items()])\n",
    "#results = predict(test_set.data, id2label)\n",
    "results = predict(data, id2label)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
